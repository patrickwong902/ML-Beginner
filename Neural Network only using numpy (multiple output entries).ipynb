{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script, we would like to build a simple, generalized neural network with n features (each input data has n values) and m outputs with only one hidden layer with k neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set is: \n",
      " [[0 0 1]\n",
      " [1 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " [0 0 0]] \n",
      "\n",
      "The output is: \n",
      " [[0 0 1]\n",
      " [1 0 0]\n",
      " [1 1 0]\n",
      " [0 1 1]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1], [0, 0, 0]])\n",
    "output = np.array([[0, 0, 1], [1, 0, 0], [1, 1, 0], [0, 1, 1], [0, 0, 1]])\n",
    "print(\"The data set is: \\n\", dataset, \"\\n\")\n",
    "print(\"The output is: \\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augment the input matrix to implement also the bias parameter on the input layer\n",
    "b1 = np.ones([dataset.shape[0], 1])\n",
    "dataset1 = np.hstack((dataset, b1))\n",
    "\n",
    "#number of neurons\n",
    "nn = 20\n",
    "\n",
    "#initialize the first weight matrix:\n",
    "W_1 = np.random.random([dataset1.shape[1], nn])\n",
    "\n",
    "#intialize the second weight matrix, consider also the bias parameter:\n",
    "W_2 = np.random.random([nn + 1, output.shape[1]])\n",
    "\n",
    "#activation function:\n",
    "def sig(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#derivative of sigmoid:\n",
    "def dsig(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "lr =  0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish with 8020 iteration(s) with error 0.009999889543683154 .\n",
      "\n",
      "The weight matrix from the input to the hidden layer: \n",
      " [[ 5.21355026e+00 -1.79035749e+00  1.38728654e+00 -9.93489007e-01\n",
      "  -4.77602437e-01  7.30297976e-01 -6.86853711e-01  3.96058359e+00\n",
      "   3.28974682e+00  1.11240501e+00  1.46095030e+00 -2.76527970e+00\n",
      "  -2.53484403e+00  2.26706921e-01 -3.91178721e+00  2.63835610e+00\n",
      "   2.43251381e+00  4.60445869e+00  3.20155361e+00  1.88556817e+00]\n",
      " [ 4.82350612e+00  1.97972131e-01  3.68655752e-01  2.90886186e-01\n",
      "   1.35471660e+00  2.14356036e-01 -1.11302545e+00  3.52720056e+00\n",
      "  -1.86162091e-01  6.79334935e-01 -4.83551197e-02  7.91421937e-01\n",
      "   3.29499691e+00  3.62829599e-01  3.28193143e-01 -1.70288073e+00\n",
      "   4.82066391e-02  4.75571187e+00 -2.13417037e+00  1.72413412e+00]\n",
      " [ 1.83828729e-01 -3.02280690e-01  6.51954863e-01  1.66633248e-01\n",
      "  -4.76223527e-03 -1.25572521e-01  1.18779078e+00  3.74149440e-01\n",
      "   9.14113589e-02 -6.09414718e-01  2.23085342e-01 -4.36922299e-02\n",
      "   4.80998910e-01 -2.68051412e-01 -2.00331328e-01  7.05804982e-02\n",
      "   2.64653288e-01 -3.56539664e+00  4.43843974e-01  1.17305594e+00]\n",
      " [-2.08494461e+00  6.77872033e-01 -7.81681610e-01 -1.23158365e-01\n",
      "  -3.69424270e-02 -4.94273646e-01  1.42170778e-01 -1.54662400e+00\n",
      "  -1.51658495e+00 -5.31013510e-01 -6.30334968e-01  7.11277715e-01\n",
      "   8.50863496e-01  4.41763545e-01  1.89264597e+00  1.09034533e+00\n",
      "  -1.32396862e+00 -3.59353460e+00  7.50732347e-01 -6.99383745e-01]]\n",
      "\n",
      "The weight matrix from the hidden to the output layer: \n",
      " [[ 0.65018674  7.02787841 -1.26637651]\n",
      " [-1.46275238 -0.26959428  1.88707666]\n",
      " [ 1.13455377  0.34876549 -0.47638503]\n",
      " [-0.65164543  0.06337571  1.15593141]\n",
      " [-0.68624834 -1.10445653  0.50632502]\n",
      " [ 0.29537801 -0.38165425 -0.36416189]\n",
      " [-0.3624431   1.58002021  0.67288619]\n",
      " [ 0.99230666  4.57655199 -0.6914549 ]\n",
      " [ 2.82627792 -0.4284673  -2.27127229]\n",
      " [ 0.64480295 -0.72356339 -0.5722689 ]\n",
      " [ 0.95087816  0.01217691 -0.98289901]\n",
      " [-2.24151774  0.37922886  2.61700628]\n",
      " [-1.07375917 -3.59605071  0.86963047]\n",
      " [-0.28102197 -0.73293529 -0.01992098]\n",
      " [-3.77200662  0.09264462  3.86966461]\n",
      " [ 0.75699514 -2.2512083  -0.11067828]\n",
      " [ 1.70463616 -0.07661779 -1.87743776]\n",
      " [ 2.04026922 -8.22734272 -1.88090085]\n",
      " [ 0.84744806 -2.65318079 -1.15189613]\n",
      " [-0.04784226  2.05554192  0.31005098]\n",
      " [-1.72302733 -1.68221213  0.47074595]]\n",
      "\n",
      "The final predicted output: \n",
      " [[0.00391092 0.00854603 0.99674026]\n",
      " [0.99712635 0.01085794 0.00277421]\n",
      " [0.99549087 0.98935444 0.00466045]\n",
      " [0.0033097  0.99073341 0.9961029 ]\n",
      " [0.00272245 0.00248505 0.99718646]]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "err = 100\n",
    "while i<100000:\n",
    "    if err < 1e-2:\n",
    "        break\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "    #update the predicted values:\n",
    "    z = sig(np.dot(dataset1, W_1))\n",
    "    \n",
    "    #augment z to include the bias parameter in the hidden layer\n",
    "    b2 = np.ones([z.shape[0], 1])\n",
    "    z = np.hstack((z, b2))\n",
    "    pred_out = sig(np.dot(z, W_2))\n",
    "    \n",
    "    #compute:\n",
    "    err = 0.5 * np.linalg.norm(output - pred_out, 2)\n",
    "    \n",
    "    #back propagation using gradient descent method:\n",
    "    dW_2 = np.dot(z.T, -(output - pred_out) * dsig(pred_out))\n",
    "    dW_1 = np.zeros_like(W_1)\n",
    "    for j in range(dW_1.shape[0]):\n",
    "        for k in range(dW_1.shape[1]):\n",
    "            dW_1[j, k] = np.dot(W_2[k,:], np.dot(-(output - pred_out).T * dsig(pred_out).T, dsig(z)[:,k] * dataset1[:,j]))\n",
    "    \n",
    "    #update the weight matrix\n",
    "    W_1 -= lr * dW_1\n",
    "    W_2 -= lr * dW_2\n",
    "    \n",
    "\n",
    "print(\"Finish with\",i, \"iteration(s) with error\", err,\".\")\n",
    "print(\"\\nThe weight matrix from the input to the hidden layer: \\n\", W_1)\n",
    "print(\"\\nThe weight matrix from the hidden to the output layer: \\n\", W_2)\n",
    "print(\"\\nThe final predicted output: \\n\", pred_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
